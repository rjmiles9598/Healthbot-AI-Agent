{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U ipykernel\n",
    "!pip install --quiet -U langchain\n",
    "!pip install --quiet -U langchain-openai\n",
    "!pip install --quiet -U langgraph\n",
    "!pip install --quiet -U langchainhub\n",
    "!pip install --quiet -U tavily-python\n",
    "!pip install --quiet -U langchain-community\n",
    "!pip install --quiet -U python-dotenv==1.0.1\n",
    "!pip install --quiet -U langchain-anthropic\n",
    "!pip install --quiet -U mlflow\n",
    "!pip install --quiet -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c194bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Any, Literal\n",
    "\n",
    "# Tavily API\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Langchain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.vectorstores import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Langgraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "# Evaluation \n",
    "import mlflow\n",
    "\n",
    "# Anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8f0414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_openai.chat_models.base.ChatOpenAI"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"config.env\")\n",
    "\n",
    "base_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "type(base_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e058466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I am based on OpenAI's GPT-3.5 model. If you have any specific questions or need assistance, feel free to ask!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 15, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_aa07c96156', 'id': 'chatcmpl-ClkStv3xREoWzBqdMDGO9Ry6N2liE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b0fc4-030f-7683-bf17-64ed9e3dee67-0' usage_metadata={'input_tokens': 15, 'output_tokens': 29, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "fdsa = base_llm.invoke(\"what chat gpt model is this?\")\n",
    "\n",
    "print(fdsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient()\n",
    "type(tavily_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d6f94",
   "metadata": {},
   "source": [
    "## Create our tool node and LLM with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our tavily search tool node\n",
    "\n",
    "@tool\n",
    "def web_search(question:str)->Dict:\n",
    "    \"\"\"\n",
    "    Return top search results for a given search query.\n",
    "    \"\"\"\n",
    "    response = tavily_client.search(question)\n",
    "    print(\"Im conducting a very important search currently...\")\n",
    "    return response\n",
    "\n",
    "type(web_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_search = web_search.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the NQ in the market?\"\n",
    "    }\n",
    ")\n",
    "type(test_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search]\n",
    "type(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = base_llm.bind_tools(tools)\n",
    "type(llm_with_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84aa2e2",
   "metadata": {},
   "source": [
    "## Create our State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    topic: Optional[str]\n",
    "    summary: Optional[str]\n",
    "    quiz_question: Optional[str]\n",
    "    quiz_options: Optional[List[str]]\n",
    "    quiz_correct_option: Optional[str]\n",
    "    patient_answer: Optional[str]\n",
    "    evaluation: Optional[Dict[str, Any]]\n",
    "    phase: Optional[\n",
    "        Literal[\n",
    "            \"ask_topic\",\n",
    "            \"searching\",\n",
    "            \"show_summary\",\n",
    "            \"waiting_ready\",\n",
    "            \"quiz_generated\",\n",
    "            \"waiting_answer\",\n",
    "            \"evaluated\",\n",
    "            \"waiting_restart\"\n",
    "        ]\n",
    "    ]\n",
    "    repeat_mode: bool\n",
    "    #Dont need a messages variable because the MessagesState autoimatically includes a messages field\n",
    "\n",
    "\n",
    "type(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bf6ac",
   "metadata": {},
   "source": [
    "## Create an entrypoint node. \n",
    "\n",
    "This node should also be the introduction of the system to the user. \n",
    "\n",
    "This node will have an interrupt after to collect the topic the user wants to learn about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc153f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create our entrypoint into our langgraph workflow.\n",
    "\n",
    "def entrypoint(state: State):\n",
    "    sys_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are a helpful assistant that conducts web searches to help patients understand their medical conditions \" \\\n",
    "            \"treatment options, and post-treatment care. \" \\\n",
    "            \"Your job is to answer patient querieswith web searches, if needed, and provide summarized medical information. \" \\\n",
    "            \"After. you provide information to the user, you will also quiz the user.\"))\n",
    "    ai_message = AIMessage(\n",
    "        content=(\n",
    "            \"What health topic or medical condition do you want to learn about?\"\n",
    "        )\n",
    "    )\n",
    "    state[\"messages\"] = state[\"messages\"] + [sys_message] + [ai_message]\n",
    "    return {\"messages\": [sys_message, ai_message], \n",
    "            \"phase\": \"ask_topic\",}\n",
    "\n",
    "\n",
    "\n",
    "# Need an interrupt after this node to be able to store my topic. This interrupt will be an input for the user to be able to input their desired topic they want to learn about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9611",
   "metadata": {},
   "source": [
    "## Create our Agent node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create our llm agent node\n",
    "\n",
    "def agent(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    messages = state[\"messages\"]\n",
    "    if topic is None:\n",
    "        messages = messages + [\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Please search the web for up-to-date patient-friendly information about {topic}. \"\n",
    "                    \"Summarize what the topic is for the patient, provide key points, and general treatment/management options, in simple english.\" \\\n",
    "                    \"This is for education only, not medical advice.\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "\n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": messages + [ai_message], \"phase\": \"searching\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    summary = state[\"messages\"][-1]\n",
    "    for m in summary:\n",
    "        m.pretty_print()\n",
    "    return {\"messages\": messages + [summary], \"phase\": \"show_summary\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_node(state: State):\n",
    "    messages = state[\"mesages\"]\n",
    "    sys_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an assistant that quizzes patients with medical conditions. Your job is to look at the previous information shared with the patient and generate a quiz for them, helping to reinforce their understanding of their condition.\"))\n",
    "    ai_message = AIMessage(\n",
    "        content=(\n",
    "            \"Are you ready to take a quiz on the information you were just provided?\"))\n",
    "    state[\"messages\"] + [sys_message + ai_message]\n",
    "    quiz_question = input(\"Yes or no: \")\n",
    "    if quiz_question.lower() == \"yes\":\n",
    "        quiz = base_llm.invoke(messages)\n",
    "        unformatted_quiz = quiz.content\n",
    "        for m in unformatted_quiz:\n",
    "            m.pretty_print()\n",
    "    else:\n",
    "        return \"END\"\n",
    "    return {\"messages\": messages + [sys_message, ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router1(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    return \"summarizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router2(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our graph workflow\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "#workflow.add_node(\"entrypoint\", entrypoint)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"tool_node\", ToolNode([web_search]))\n",
    "workflow.add_node(\"summarizer\", summarizer)\n",
    "workflow.add_node(\"quiz_node\", quiz_node)\n",
    "##workflow.add_node(\"node_2\", ToolNode([web_search]))\n",
    "#workflow.add_node(\"node_3\", )\n",
    "#workflow.add_node(\"node_4\", )\n",
    "#workflow.add_node(\"node_5\", )\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "#workflow.add_edge(\"agent\", \"first_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source= \"agent\",\n",
    "    path=router1,\n",
    "    path_map= [\"tool_node\", \"summarizer\"]\n",
    ")\n",
    "\n",
    "#workflow.add_conditional_edges(\n",
    "    #source=\"presenter_node\",\n",
    "   #path=continue_router,\n",
    "    #path_map=[\"entrypoint\", \"END\"]\n",
    "#)\n",
    "\n",
    "workflow.add_edge(\"tool_node\", \"agent\")\n",
    "workflow.add_edge(\"summarizer\", \"quiz_node\")\n",
    "workflow.add_edge(\"quiz_node\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = workflow.compile()\n",
    "#memory = MemorySaver()\n",
    "#graph = workflow.compile(\n",
    "#    interrupt_after=[\"entrypoint\"],\n",
    "#    checkpointer=memory\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7386294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_in_loop_ask_topic(graph: CompiledStateGraph, question:str, thread_id:int):\n",
    "    topic = {\"topic\": topic}\n",
    "    #config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    for event in graph.stream(input=topic, stream_mode=\"values\"):\n",
    "        if not event['messages']:\n",
    "            continue\n",
    "        event['messages'][-1].pretty_print()\n",
    "\n",
    "    human_input = input(\"Do you approve the tool calling(Y or N)?: \")\n",
    "        \n",
    "    if human_input.lower() == \"yes\":\n",
    "        for event in graph.stream(input=None, stream_mode=\"values\"):\n",
    "            if not event['messages']:\n",
    "                continue\n",
    "            event['messages'][-1].pretty_print()\n",
    "    else:\n",
    "        AIMessage(\"Please provide a topic that you want to learn about.\").pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47430c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = graph.invoke(\n",
    "    {\"topic\": \"What is COPD?\"}\n",
    ")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04725085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in test[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
