{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806f900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U ipykernel\n",
    "!pip install --quiet -U langchain\n",
    "!pip install --quiet -U langchain-openai\n",
    "!pip install --quiet -U langgraph\n",
    "!pip install --quiet -U langchainhub\n",
    "!pip install --quiet -U tavily-python\n",
    "!pip install --quiet -U langchain-community\n",
    "!pip install --quiet -U python-dotenv==1.0.1\n",
    "!pip install --quiet -U langchain-anthropic\n",
    "!pip install --quiet -U mlflow\n",
    "!pip install --quiet -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c194bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmiles/Desktop/local_projects/Python_for_AI/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Any, Literal\n",
    "\n",
    "# Tavily API\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Langchain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.vectorstores import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Langgraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "# Evaluation \n",
    "import mlflow\n",
    "\n",
    "# Anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"config.env\")\n",
    "\n",
    "base_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904d748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d6f94",
   "metadata": {},
   "source": [
    "## Create our tool node and LLM with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our tavily search tool node\n",
    "\n",
    "@tool\n",
    "\n",
    "# Here we define our function with an input \"question\" and hint that its a string. the operator \"->\" provides a hint that the return type is a Dict and should be JSON-like\n",
    "def web_search(question:str)->Dict:\n",
    "    \"\"\"\n",
    "    Return top search results for a given search query.\n",
    "    \"\"\"\n",
    "    # Below we call Tavily's software development kit's (SDK) .search() and provide the input given the the function which should be a user inputted string.\n",
    "        # This performs a synchronous HTTPS request using the API key bound to my 'tavily_client' that i have instantiated already\n",
    "        # the above call to tavily's SDK and its function to perform the HTTPS request with the API key gets stored in the 'response' object.\n",
    "    response = tavily_client.search(question)\n",
    "    print(\"im at the tool node\")\n",
    "    # below we return our object which should be a Dict. \n",
    "    # In an agent run, LangChain serializes this to JSON and passes it back to the model as a ToolMessage so the model can read and use the results for the next reasoning step.\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be30402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b985f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = base_llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84aa2e2",
   "metadata": {},
   "source": [
    "## Create our State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    topic: Optional[str]\n",
    "    summary: Optional[str]\n",
    "    quiz_question: Optional[str]\n",
    "    quiz_options: Optional[List[str]]\n",
    "    quiz_correct_option: Optional[str]\n",
    "    patient_answer: Optional[str]\n",
    "    evaluation: Optional[Dict[str, Any]]\n",
    "    phase: Optional[\n",
    "        Literal[\n",
    "            \"ask_topic\",\n",
    "            \"searching\",\n",
    "            \"show_summary\",\n",
    "            \"waiting_ready\",\n",
    "            \"quiz_generated\",\n",
    "            \"waiting_answer\",\n",
    "            \"evaluated\",\n",
    "            \"waiting_restart\"\n",
    "        ]\n",
    "    ]\n",
    "    repeat_mode: bool\n",
    "    #Dont need a messages variable because the MessagesState autoimatically includes a messages field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bf6ac",
   "metadata": {},
   "source": [
    "## Create an entrypoint node. \n",
    "\n",
    "This node should also be the introduction of the system to the user. \n",
    "\n",
    "This node will have an interrupt after to collect the topic the user wants to learn about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc153f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create our entrypoint into our langgraph workflow.\n",
    "\n",
    "def entrypoint(state: State):\n",
    "    sys_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are a helpful assistant that conducts web searches to help patients understand their medical conditions \" \\\n",
    "            \"treatment options, and post-treatment care. \" \\\n",
    "            \"Your job is to answer patient querieswith web searches, if needed, and provide summarized medical information. \" \\\n",
    "            \"After. you provide information to the user, you will also quiz the user.\"))\n",
    "    ai_message = AIMessage(\n",
    "        content=(\n",
    "            \"What health topic or medical condition do you want to learn about?\"\n",
    "        )\n",
    "    )\n",
    "    state[\"messages\"] = state[\"messages\"] + [sys_message] + [ai_message]\n",
    "    return {\"messages\": [sys_message, ai_message], \n",
    "            \"phase\": \"ask_topic\",}\n",
    "\n",
    "\n",
    "\n",
    "# Need an interrupt after this node to be able to store my topic. This interrupt will be an input for the user to be able to input their desired topic they want to learn about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9611",
   "metadata": {},
   "source": [
    "## Create our Agent node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create our llm agent node\n",
    "\n",
    "def agent(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    messages = state[\"messages\"]\n",
    "    if topic is None:\n",
    "        messages = messages + [\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Please search the web for up-to-date patient-friendly information about {topic}. \"\n",
    "                    \"Summarize what the topic is for the patient, provide key points, and general treatment/management options, in simple english.\" \\\n",
    "                    \"This is for education only, not medical advice.\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "\n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": messages + [ai_message], \"phase\": \"searching\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    summary = state[\"messages\"][-1]\n",
    "    for m in summary:\n",
    "        m.pretty_print()\n",
    "    return {\"messages\": messages + [summary], \"phase\": \"show_summary\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_node(state: State):\n",
    "    messages = state[\"mesages\"]\n",
    "    sys_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an assistant that quizzes patients with medical conditions. Your job is to look at the previous information shared with the patient and generate a quiz for them, helping to reinforce their understanding of their condition.\"))\n",
    "    ai_message = AIMessage(\n",
    "        content=(\n",
    "            \"Are you ready to take a quiz on the information you were just provided?\"))\n",
    "    state[\"messages\"] + [sys_message + ai_message]\n",
    "    quiz_question = input(\"Yes or no: \")\n",
    "    if quiz_question.lower() == \"yes\":\n",
    "        quiz = base_llm.invoke(messages)\n",
    "        unformatted_quiz = quiz.content\n",
    "        for m in unformatted_quiz:\n",
    "            m.pretty_print()\n",
    "    else:\n",
    "        return \"END\"\n",
    "    return {\"messages\": messages + [sys_message, ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router1(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    return \"summarizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router2(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our graph workflow\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "#workflow.add_node(\"entrypoint\", entrypoint)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"tool_node\", ToolNode([web_search]))\n",
    "workflow.add_node(\"summarizer\", summarizer)\n",
    "workflow.add_node(\"quiz_node\", quiz_node)\n",
    "##workflow.add_node(\"node_2\", ToolNode([web_search]))\n",
    "#workflow.add_node(\"node_3\", )\n",
    "#workflow.add_node(\"node_4\", )\n",
    "#workflow.add_node(\"node_5\", )\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "#workflow.add_edge(\"agent\", \"first_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source= \"agent\",\n",
    "    path=router1,\n",
    "    path_map= [\"tool_node\", \"summarizer\"]\n",
    ")\n",
    "\n",
    "#workflow.add_conditional_edges(\n",
    "    #source=\"presenter_node\",\n",
    "   #path=continue_router,\n",
    "    #path_map=[\"entrypoint\", \"END\"]\n",
    "#)\n",
    "\n",
    "workflow.add_edge(\"tool_node\", \"agent\")\n",
    "workflow.add_edge(\"summarizer\", \"quiz_node\")\n",
    "workflow.add_edge(\"quiz_node\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = workflow.compile()\n",
    "#memory = MemorySaver()\n",
    "#graph = workflow.compile(\n",
    "#    interrupt_after=[\"entrypoint\"],\n",
    "#    checkpointer=memory\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7386294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_in_loop_ask_topic(graph: CompiledStateGraph, question:str, thread_id:int):\n",
    "    topic = {\"topic\": topic}\n",
    "    #config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    for event in graph.stream(input=topic, stream_mode=\"values\"):\n",
    "        if not event['messages']:\n",
    "            continue\n",
    "        event['messages'][-1].pretty_print()\n",
    "\n",
    "    human_input = input(\"Do you approve the tool calling(Y or N)?: \")\n",
    "        \n",
    "    if human_input.lower() == \"yes\":\n",
    "        for event in graph.stream(input=None, stream_mode=\"values\"):\n",
    "            if not event['messages']:\n",
    "                continue\n",
    "            event['messages'][-1].pretty_print()\n",
    "    else:\n",
    "        AIMessage(\"Please provide a topic that you want to learn about.\").pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47430c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = graph.invoke(\n",
    "    {\"topic\": \"What is COPD?\"}\n",
    ")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04725085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in test[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
