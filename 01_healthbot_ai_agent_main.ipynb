{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Any, Literal\n",
    "\n",
    "# Tavily API\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Langchain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.vectorstores import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# LangChain Anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Langgraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# Evaluation \n",
    "import mlflow\n",
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics.collections import Faithfulness, FactualCorrectness, ToolCallAccuracy, SemanticSimilarity\n",
    "from ragas import messages as ragas_messages\n",
    "from ragas.integrations.langgraph import convert_to_ragas_messages\n",
    "from ragas.dataset_schema import MultiTurnSample\n",
    "\n",
    "# Anthropic\n",
    "from anthropic import Anthropic, AsyncAnthropic\n",
    "\n",
    "#OpenAI\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# langfuse\n",
    "from langfuse import get_client, observe\n",
    "\n",
    "#local Imports\n",
    "#import healthbot_state\n",
    "#import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"config.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96970ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_llm = ChatOpenAI(\n",
    "    model=\"gpt-5.1\",\n",
    "    temperature=0.0,\n",
    "    verbosity=\"low\",\n",
    "    reasoning_effort=\"low\",\n",
    ")\n",
    "\n",
    "small_llm = ChatOpenAI(\n",
    "    model = \"gpt-5-nano\",\n",
    "    temperature = 0.3,\n",
    "    verbosity=\"low\",\n",
    "    reasoning_effort=\"medium\",\n",
    ")\n",
    "\n",
    "quiz_grader = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d6f94",
   "metadata": {},
   "source": [
    "## Create our tool node and LLM with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "@observe(as_type=\"tool\", capture_input=True, capture_output=True)\n",
    "def web_search(question:str)->Dict:\n",
    "    \"\"\"\n",
    "    Return top search results for a given search query.\n",
    "    \"\"\"\n",
    "    response = tavily_client.search(question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = base_llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84aa2e2",
   "metadata": {},
   "source": [
    "## Create our State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    topic: Optional[str]\n",
    "    summary: Optional[str]\n",
    "    quiz_question: Optional[str]\n",
    "    patient_answer: Optional[str]\n",
    "    evaluation: Optional[Dict[str, Any]]\n",
    "    phase: Optional[\n",
    "        Literal[\n",
    "            \"ask_topic\",\n",
    "            \"searching\",\n",
    "            \"show_summary\",\n",
    "            \"waiting_ready\",\n",
    "            \"quiz_generated\",\n",
    "            \"waiting_answer\",\n",
    "            \"evaluated\",\n",
    "            \"waiting_restart\"\n",
    "        ]\n",
    "    ]\n",
    "    repeat_mode: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bf6ac",
   "metadata": {},
   "source": [
    "## Create an entrypoint node. \n",
    "\n",
    "This node should also be the introduction of the system to the user. \n",
    "\n",
    "This node will have an interrupt after to collect the topic the user wants to learn about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc153f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrypoint(state: State)->State:\n",
    "    print(\"Hi Im the Healthbot Assistant, here to help you understand your medical conditions, treatment\\n\" \\\n",
    "          \"options, and your post-treatment care instructions. I can answer any health related questions\\n\" \\\n",
    "          \"you have, and I will ensure I to aid you in understanding your post-treatment process..\\n\")\n",
    "    sys_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are the Healthbot Assistant. You help patients understand their diagnoses, conditions,\" \\\n",
    "            \"treatment options, and provide them post-treatment care instructions. You only answer health related \" \\\n",
    "            \"questions from the patient.\" \\\n",
    "            \"Never share information that is not helpful. Helpful responses are only responses that assist users \" \\\n",
    "            \"in understanding their diagnoses and the post treatment which they should understand to have the best recovery conditions. \" \\\n",
    "            \"At no point should you ask the user for more information about anything.\"\n",
    "        )\n",
    "    )\n",
    "    ai_message = AIMessage(\n",
    "        content=(\n",
    "            \"What health topic or medical condition do you want to learn about?\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    messages = [sys_message, ai_message]\n",
    "   \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9611",
   "metadata": {},
   "source": [
    "## Create our Agent nodes\n",
    "\n",
    " - info_agent: uses a higher tier model to gather relevant data into a larger report based on the patients interest\n",
    " - summary_agent: uses a smaller model with a subset of the state[\"messages\"] list to process less tokens and only summarize the report generated by the large model. this should somewhat limit token usage and since summarization is a simpler task, we do not need to pass the same large report through a model that costs more during inference.\n",
    " - quiz_agent: uses the same smaller model to generate our quiz, again this is a simpler task where we can save on cost.\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_agent(state: State):\n",
    "    raw_topic = interrupt(\"What topic do you want to learn about? \")\n",
    "    topic = raw_topic.get('topic')\n",
    "    topic_msg = HumanMessage(content=topic)\n",
    "    messages = state[\"messages\"]\n",
    "    messages = messages + [topic_msg]\n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    messages = messages + [ai_message]\n",
    "\n",
    "    return {\"topic\": topic, \"messages\": messages, \"phase\": \"searching\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49735aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_agent(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    sys_message = SystemMessage(\n",
    "            content = (\n",
    "                \"Please summarize the following into about 3-4 paragraphs. Be concise and provide the most important information to the patient.\" \\\n",
    "                \"Do not exceed 200 words. Please make each paragraph a compact block of text with about 20 words per line and a \\n\\n in between each paragraph.\"\n",
    "            )\n",
    "        )\n",
    "    quick_message = [sys_message] + [last_message]\n",
    "    summary = small_llm.invoke(quick_message)\n",
    "    messages.remove(last_message)\n",
    "    messages = messages + [summary] \n",
    "    \n",
    "\n",
    "    return{\"messages\": messages, \"summary\": summary.content, \"repeat_mode\": False, \"phase\": \"show_summary\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ed5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################\n",
    "# Going to need a router to our tool node or next node\n",
    "##################################################################################################################################################\n",
    "# CHANGE: router will route to entrypoint if theres no tool call\n",
    "##################################################################################################################################################\n",
    "\n",
    "def router_1(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_agent(state: State):\n",
    "    #if not state[\"repeat_mode\"]:\n",
    "    summary = state[\"summary\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    quiz_sys_message = SystemMessage(\n",
    "        content=(\"Analyze the following summary:\"\n",
    "        f\"\\nHere is the summary:\\n>> {summary}\"\n",
    "        \"\\n\\nGenerate an open-ended quiz for a patient. The quiz should only be one question.\"\n",
    "        \"\\nMake this quiz question level 2 difficulty on a scale of 0 to 5. \"\n",
    "        \"\\n0 = kindergaten level difficulty. 5 = highschool level diffuculty. \" \\\n",
    "        \"\\nThis quiz question should be based only on the summary provided.\" \\\n",
    "        \"\\nThis quiz question will be presented to a patient. Its your goal to help the patient understand their post treatment care provided in the summary. \" \\\n",
    "        \"\\nOnly generate the described one quiz question, nothing else.\" ))\n",
    "    \n",
    "    quiz_question = small_llm.invoke([quiz_sys_message]) \n",
    "    \n",
    "    messages = messages + [quiz_sys_message, quiz_question]\n",
    "\n",
    "    return {\"messages\": messages, \"quiz_question\": quiz_question.content, \"phase\": \"waiting_answer\"}\n",
    "    #else:\n",
    "        #return {\"messages\": messages, \"quiz_question\": quiz_question.content, \"phase\": \"waiting_answer\"}, print(quiz_question.content)\n",
    "    \n",
    "\n",
    "# Human in the loop after the quiz is presented to collect the patients answer to the quiz question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def quiz_grader_agent(state: State):\n",
    "    # Interrupt to gather the patients answer, reinvoking in the HITL will store the value as the patient answer\n",
    "    patient_answer = interrupt(\"Please answer the quiz question: \")\n",
    "    \n",
    "    client = AsyncOpenAI()\n",
    "    messages = state[\"messages\"]\n",
    "    summary = state[\"summary\"]\n",
    "    quiz_question = state[\"quiz_question\"]\n",
    "    \n",
    "    evaluator_llm = llm_factory(model=\"gpt-5-nano\", provider=\"openai\", client=client)\n",
    "\n",
    "    scorer = Faithfulness(llm=evaluator_llm)\n",
    "\n",
    "    result = await scorer.ascore(\n",
    "        user_input = quiz_question, \n",
    "        response = patient_answer, \n",
    "        retrieved_contexts = [summary])\n",
    "\n",
    "    result_str = str(result)\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You generates a summary of post treatment care for a patient. Then you created a quiz for the patient.\"\n",
    "            \"\\nThis is the quiz: \\n>>{quiz_question}. \"\n",
    "            \"\\n\\nThe patient provided this answer to the quiz:\\n{patient_answer}\"\n",
    "            \"\\nThe patients answer had a faithfullness score of {result} to the quiz question.\"\n",
    "            \"\\nThe value of the faithfullness score is on a range between 0 and 1. anything that is a 0.6 and below is a fail. Anything above a 0.6 is a pass.\"\n",
    "            \"\\n\\nTranslate the patients score to a pass or fail grading. Output only, 'PASS' or 'FAIL' based on the score. Do not output anything else. \"),\n",
    "            (\"human\",\"What did I make on the quiz?\"),])\n",
    "\n",
    "    ai_message = prompt_template.invoke({\n",
    "        \"patient_answer\": patient_answer,\n",
    "        \"quiz_question\": quiz_question, \n",
    "        \"result\": result_str,\n",
    "    })\n",
    "\n",
    "    evaluation = await small_llm.ainvoke(ai_message.to_messages())\n",
    "\n",
    "    messages = messages + [HumanMessage(content=patient_answer), ai_message.to_messages(), evaluation]\n",
    "   \n",
    "    print(result_str)\n",
    "    print(evaluation.content)\n",
    "    return {\"mesages\": messages, \"patient_answer\": patient_answer, \"evaluation\": evaluation.content, \"phase\": \"evaluated\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eab398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_2(state: State):\n",
    "    repeat_mode = state[\"repeat_mode\"]\n",
    "    if repeat_mode:\n",
    "        return \"quiz_agent\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the quiz_agent, summary agent, info_gather_agent with tool calling workflow.\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"entrypoint\", entrypoint)\n",
    "workflow.add_node(\"info_agent\", info_agent)\n",
    "workflow.add_node(\"tools\", ToolNode([web_search]))\n",
    "workflow.add_node(\"summary_agent\", summary_agent)\n",
    "workflow.add_node(\"quiz_agent\", quiz_agent)\n",
    "workflow.add_node(\"quiz_grader_agent\", quiz_grader_agent)\n",
    "\n",
    "workflow.add_edge(START, \"entrypoint\")\n",
    "workflow.add_edge(\"entrypoint\", \"info_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source = \"info_agent\",\n",
    "    path = router_1,\n",
    "    path_map = [\"tools\", END]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"info_agent\")\n",
    "workflow.add_edge(\"info_agent\", \"summary_agent\")\n",
    "workflow.add_edge(\"summary_agent\", \"quiz_agent\")\n",
    "workflow.add_edge(\"quiz_agent\", \"quiz_grader_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"quiz_grader_agent\",\n",
    "    path=router_2,\n",
    "    path_map=[\"quiz_agent\", END]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"quiz_grader_agent\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be786937",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = workflow.compile(\n",
    "    checkpointer = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15772866",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc34827",
   "metadata": {},
   "source": [
    "### Human in the loop\n",
    "\n",
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7951c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langfuse_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def hitl_interaction_flow(graph: CompiledStateGraph, thread_id: int): \n",
    "    #Interrupt after the entrypoint node\n",
    "    #topic = {\"topic\": human_input_topic}\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}, \n",
    "              \"callbacks\": [langfuse_handler],\n",
    "              \"metadata\":{\n",
    "                  \"session_id\": str(thread_id),\n",
    "                  \"app\": \"Healthbot\",\n",
    "              }}\n",
    "\n",
    "    #restart_from = graph.get_state(config)     #store a checkpoint from this step\n",
    "    async def run_until_interrupt_or_end(resume_payload=None):\n",
    "        stream_input = Command(resume=resume_payload) if resume_payload is not None else {}\n",
    "\n",
    "        async for update in graph.astream(stream_input, config=config, stream_mode=\"updates\"):\n",
    "            for _, node_out in update.items():\n",
    "                if isinstance(node_out, dict) and node_out.get(\"messages\"):\n",
    "                    node_out[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            if \"__interrupt__\" in update:\n",
    "                return update[\"__interrupt__\"]\n",
    "            if \"interrupt\" in update:\n",
    "                return update[\"interrupt\"]\n",
    "        return None\n",
    "        \n",
    "    interrupt_payload = await run_until_interrupt_or_end()\n",
    "    if interrupt_payload is None:\n",
    "        return\n",
    "    \n",
    "    human_input_topic = input(\"Please input what post treatment topic you'd like to learn about: \").strip()\n",
    "    resume_1 = {\n",
    "        \"topic\": human_input_topic,\n",
    "        \"messages\": [HumanMessage(content=human_input_topic)],\n",
    "    }\n",
    "\n",
    "    interrupt_payload = await run_until_interrupt_or_end(resume_payload=resume_1)\n",
    "    if interrupt_payload is None:\n",
    "        return  # graph ended early (quiz interrupt never happened)\n",
    "\n",
    "    quiz_answer_input = input(\"Please provide an answer to the quiz below:\\n>> \").strip()\n",
    "    resume_2 = {\n",
    "        \"patient_answer\": quiz_answer_input,\n",
    "        \"messages\": [HumanMessage(content=quiz_answer_input)],\n",
    "    }\n",
    "\n",
    "    await run_until_interrupt_or_end(resume_payload=resume_2[\"patient_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('config.env')\n",
    "langfuse = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "await hitl_interaction_flow(\n",
    "    graph=graph,\n",
    "    thread_id=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
